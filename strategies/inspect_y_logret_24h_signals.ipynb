{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inspect y_logret_24h signals\n",
        "\n",
        "This notebook loads train/val/test signal CSVs produced by `strategies.compute_signals_from_quantiles` for `BINANCE_BTCUSDT.P, 60`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure project root is on sys.path for 'strategies' imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd().parents[0]\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "# Quick import check (optional)\n",
        "try:\n",
        "    from strategies.quantile_signals import estimate_prob_up\n",
        "except Exception as e:\n",
        "    print(\"Import warning:\", e)\n",
        "    # Fallback if needed: from quantile_signals import estimate_prob_up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded:\n",
            "train: (15453, 20) /Volumes/Extreme SSD/trading_data/cex/models/BINANCE_BTCUSDT.P, 60/diagnosis/y_logret_120h/pred_train_signals.csv\n",
            "val: (3311, 20) /Volumes/Extreme SSD/trading_data/cex/models/BINANCE_BTCUSDT.P, 60/diagnosis/y_logret_120h/pred_val_signals.csv\n",
            "test: (3313, 20) /Volumes/Extreme SSD/trading_data/cex/models/BINANCE_BTCUSDT.P, 60/diagnosis/y_logret_120h/pred_test_signals.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(            timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2023-01-31 00:00:00  0.020052 -0.076704 -0.051411  0.014310  0.014310   \n",
              " 1 2023-01-31 01:00:00  0.021190 -0.076704 -0.051411  0.014586  0.014586   \n",
              " 2 2023-01-31 02:00:00  0.018952 -0.076704 -0.051411  0.014674  0.014674   \n",
              " 3 2023-01-31 03:00:00  0.022383 -0.076704 -0.051411  0.015676  0.015676   \n",
              " 4 2023-01-31 04:00:00  0.021943 -0.076704 -0.051411  0.012701  0.012701   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  pred_q95  cross_violations  \\\n",
              " 0  0.014310  0.021838  0.024044   0.08092  0.109214                 1   \n",
              " 1  0.014586  0.021838  0.024044   0.08092  0.109214                 1   \n",
              " 2  0.014674  0.021838  0.024044   0.08092  0.109214                 1   \n",
              " 3  0.015676  0.021838  0.024044   0.08092  0.109214                 1   \n",
              " 4  0.012701  0.022181  0.024044   0.08092  0.109214                 1   \n",
              " \n",
              "    max_cross_gap  exp_ret_avg  exp_ret_conservative   prob_up  tail_spread  \\\n",
              " 0       0.032117     0.016694              0.011238  0.860887     0.185918   \n",
              " 1       0.032393     0.016832              0.011404  0.861051     0.185918   \n",
              " 2       0.032481     0.016876              0.011456  0.861102     0.185918   \n",
              " 3       0.033483     0.017377              0.012057  0.861683     0.185918   \n",
              " 4       0.030508     0.015949              0.010306  0.859905     0.185918   \n",
              " \n",
              "    signal_exp  signal_prob  signal_quantile  \n",
              " 0           1            1                0  \n",
              " 1           1            1                0  \n",
              " 2           1            1                0  \n",
              " 3           1            1                0  \n",
              " 4           1            1                0  ,\n",
              "             timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2024-11-04 21:00:00  0.130250 -0.077052 -0.053408  0.074706  0.074706   \n",
              " 1 2024-11-04 22:00:00  0.119305 -0.077052 -0.053408  0.067782  0.067782   \n",
              " 2 2024-11-04 23:00:00  0.122509 -0.077052 -0.053408  0.073519  0.073519   \n",
              " 3 2024-11-05 00:00:00  0.122598 -0.077052 -0.053408  0.064583  0.064583   \n",
              " 4 2024-11-05 01:00:00  0.120685 -0.077052 -0.053408  0.061872  0.061872   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  pred_q95  cross_violations  \\\n",
              " 0  0.074706  0.074706  0.095152  0.095152  0.106853                 2   \n",
              " 1  0.067782  0.072415  0.093625  0.093625  0.106853                 2   \n",
              " 2  0.073519  0.073519  0.093625  0.093625  0.106853                 2   \n",
              " 3  0.064583  0.064583  0.076849  0.082645  0.106853                 1   \n",
              " 4  0.061872  0.061872  0.076849  0.082645  0.106853                 1   \n",
              " \n",
              "    max_cross_gap  exp_ret_avg  exp_ret_conservative   prob_up  tail_spread  \\\n",
              " 0       0.094295     0.061885              0.056776  0.879156     0.183904   \n",
              " 1       0.087371     0.057832              0.052240  0.877965     0.183904   \n",
              " 2       0.093108     0.060893              0.055793  0.878961     0.183904   \n",
              " 3       0.084172     0.053054              0.048150  0.877368     0.183904   \n",
              " 4       0.081461     0.051224              0.046252  0.876835     0.183904   \n",
              " \n",
              "    signal_exp  signal_prob  signal_quantile  \n",
              " 0           1            1                1  \n",
              " 1           1            1                1  \n",
              " 2           1            1                1  \n",
              " 3           1            1                0  \n",
              " 4           1            1                0  ,\n",
              "             timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2025-03-22 20:00:00  0.039409 -0.080557 -0.067324 -0.064692 -0.029207   \n",
              " 1 2025-03-22 21:00:00  0.041341 -0.080557 -0.067324 -0.062127 -0.029207   \n",
              " 2 2025-03-22 22:00:00  0.039506 -0.080557 -0.067324 -0.065223 -0.029207   \n",
              " 3 2025-03-22 23:00:00  0.039612 -0.080557 -0.067324 -0.067320 -0.029207   \n",
              " 4 2025-03-23 00:00:00  0.039097 -0.080557 -0.067324 -0.063027 -0.029207   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  pred_q95  cross_violations  \\\n",
              " 0  0.000369  0.026686  0.053891  0.084143  0.106853                 0   \n",
              " 1  0.000369  0.026686  0.052900  0.084143  0.106853                 0   \n",
              " 2  0.000369  0.026686  0.055688  0.084143  0.106853                 0   \n",
              " 3  0.000369  0.026686  0.055688  0.084143  0.106853                 0   \n",
              " 4 -0.000289  0.025748  0.061642  0.084143  0.106853                 0   \n",
              " \n",
              "    max_cross_gap  exp_ret_avg  exp_ret_conservative   prob_up  tail_spread  \\\n",
              " 0            0.0     0.001654             -0.010188  0.503116     0.187409   \n",
              " 1            0.0     0.001772             -0.009981  0.503116     0.187409   \n",
              " 2            0.0     0.001749             -0.010151  0.503116     0.187409   \n",
              " 3            0.0     0.001592             -0.010361  0.503116     0.187409   \n",
              " 4            0.0     0.002032             -0.009892  0.497228     0.187409   \n",
              " \n",
              "    signal_exp  signal_prob  signal_quantile  \n",
              " 0           1            0                1  \n",
              " 1           1            0                1  \n",
              " 2           1            0                1  \n",
              " 3           1            0                1  \n",
              " 4           1            0                1  )"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "root = Path(\"/Volumes/Extreme SSD/trading_data/cex/models/BINANCE_BTCUSDT.P, 60/diagnosis/y_logret_120h\")\n",
        "train_csv = root / \"pred_train_signals.csv\"\n",
        "val_csv = root / \"pred_val_signals.csv\"\n",
        "test_csv = root / \"pred_test_signals.csv\"\n",
        "\n",
        "# Load\n",
        "train_df = pd.read_csv(train_csv)\n",
        "val_df = pd.read_csv(val_csv)\n",
        "test_df = pd.read_csv(test_csv)\n",
        "\n",
        "for df in (train_df, val_df, test_df):\n",
        "    if 'timestamp' in df.columns:\n",
        "        ts = pd.to_datetime(df['timestamp'], errors='coerce', utc=True)\n",
        "        df['timestamp'] = ts.dt.tz_convert('UTC').dt.tz_localize(None)\n",
        "\n",
        "print(\"Loaded:\")\n",
        "print(\"train:\", train_df.shape, train_csv)\n",
        "print(\"val:\", val_df.shape, val_csv)\n",
        "print(\"test:\", test_df.shape, test_csv)\n",
        "\n",
        "train_df.head(), val_df.head(), test_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded originals:\n",
            "train: (15453, 11) /Volumes/Extreme SSD/trading_data/cex/models/BINANCE_BTCUSDT.P, 60/diagnosis/y_logret_120h/pred_train.csv\n",
            "val: (3311, 11) /Volumes/Extreme SSD/trading_data/cex/models/BINANCE_BTCUSDT.P, 60/diagnosis/y_logret_120h/pred_val.csv\n",
            "test: (3313, 11) /Volumes/Extreme SSD/trading_data/cex/models/BINANCE_BTCUSDT.P, 60/diagnosis/y_logret_120h/pred_test.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(            timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2023-01-31 00:00:00  0.020052 -0.076704 -0.051411  0.014310 -0.017807   \n",
              " 1 2023-01-31 01:00:00  0.021190 -0.076704 -0.051411  0.014586 -0.017807   \n",
              " 2 2023-01-31 02:00:00  0.018952 -0.076704 -0.051411  0.014674 -0.017807   \n",
              " 3 2023-01-31 03:00:00  0.022383 -0.076704 -0.051411  0.015676 -0.017807   \n",
              " 4 2023-01-31 04:00:00  0.021943 -0.076704 -0.051411  0.012701 -0.017807   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  pred_q95  \n",
              " 0 -0.000115  0.021838  0.024044   0.08092  0.109214  \n",
              " 1 -0.000115  0.021838  0.024044   0.08092  0.109214  \n",
              " 2 -0.000530  0.021838  0.024044   0.08092  0.109214  \n",
              " 3 -0.000530  0.021838  0.024044   0.08092  0.109214  \n",
              " 4 -0.000530  0.022181  0.024044   0.08092  0.109214  ,\n",
              "             timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2024-11-04 21:00:00  0.130250 -0.077052 -0.053408  0.074706 -0.019589   \n",
              " 1 2024-11-04 22:00:00  0.119305 -0.077052 -0.053408  0.067782 -0.019589   \n",
              " 2 2024-11-04 23:00:00  0.122509 -0.077052 -0.053408  0.073519 -0.019589   \n",
              " 3 2024-11-05 00:00:00  0.122598 -0.077052 -0.053408  0.064583 -0.019589   \n",
              " 4 2024-11-05 01:00:00  0.120685 -0.077052 -0.053408  0.061872 -0.019589   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  pred_q95  \n",
              " 0  0.020667  0.072415  0.095152  0.082645  0.106853  \n",
              " 1  0.020667  0.072415  0.093625  0.082645  0.106853  \n",
              " 2  0.020667  0.072415  0.093625  0.082645  0.106853  \n",
              " 3  0.015703  0.059991  0.076849  0.082645  0.106853  \n",
              " 4  0.015703  0.059991  0.076849  0.082645  0.106853  ,\n",
              "             timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2025-03-22 20:00:00  0.039409 -0.080557 -0.067324 -0.064692 -0.029207   \n",
              " 1 2025-03-22 21:00:00  0.041341 -0.080557 -0.067324 -0.062127 -0.029207   \n",
              " 2 2025-03-22 22:00:00  0.039506 -0.080557 -0.067324 -0.065223 -0.029207   \n",
              " 3 2025-03-22 23:00:00  0.039612 -0.080557 -0.067324 -0.067320 -0.029207   \n",
              " 4 2025-03-23 00:00:00  0.039097 -0.080557 -0.067324 -0.063027 -0.029207   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  pred_q95  \n",
              " 0  0.000369  0.026686  0.053891  0.084143  0.106853  \n",
              " 1  0.000369  0.026686  0.052900  0.084143  0.106853  \n",
              " 2  0.000369  0.026686  0.055688  0.084143  0.106853  \n",
              " 3  0.000369  0.026686  0.055688  0.084143  0.106853  \n",
              " 4 -0.000289  0.025748  0.061642  0.084143  0.106853  )"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load original merged predictions (no signals)\n",
        "orig_train_csv = root / \"pred_train.csv\"\n",
        "orig_val_csv = root / \"pred_val.csv\"\n",
        "orig_test_csv = root / \"pred_test.csv\"\n",
        "\n",
        "orig_train_df = pd.read_csv(orig_train_csv)\n",
        "orig_val_df = pd.read_csv(orig_val_csv)\n",
        "orig_test_df = pd.read_csv(orig_test_csv)\n",
        "\n",
        "for df in (orig_train_df, orig_val_df, orig_test_df):\n",
        "    if 'timestamp' in df.columns:\n",
        "        ts = pd.to_datetime(df['timestamp'], errors='coerce', utc=True)\n",
        "        df['timestamp'] = ts.dt.tz_convert('UTC').dt.tz_localize(None)\n",
        "\n",
        "print(\"Loaded originals:\")\n",
        "print(\"train:\", orig_train_df.shape, orig_train_csv)\n",
        "print(\"val:\", orig_val_df.shape, orig_val_csv)\n",
        "print(\"test:\", orig_test_df.shape, orig_test_csv)\n",
        "\n",
        "orig_train_df.head(), orig_val_df.head(), orig_test_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indicator value counts (train/val/test):\n",
            "signal_prob_exp_q50\n",
            "0    7440\n",
            "1    8013\n",
            "Name: count, dtype: int64\n",
            "signal_prob_exp_q50\n",
            "-1       1\n",
            " 0    3141\n",
            " 1     169\n",
            "Name: count, dtype: int64\n",
            "signal_prob_exp_q50\n",
            "0    3282\n",
            "1      31\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(            timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2023-01-31 00:00:00  0.020052 -0.076704 -0.051411  0.014310  0.014310   \n",
              " 1 2023-01-31 01:00:00  0.021190 -0.076704 -0.051411  0.014586  0.014586   \n",
              " 2 2023-01-31 02:00:00  0.018952 -0.076704 -0.051411  0.014674  0.014674   \n",
              " 3 2023-01-31 03:00:00  0.022383 -0.076704 -0.051411  0.015676  0.015676   \n",
              " 4 2023-01-31 04:00:00  0.021943 -0.076704 -0.051411  0.012701  0.012701   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  ...  max_cross_gap  exp_ret_avg  \\\n",
              " 0  0.014310  0.021838  0.024044   0.08092  ...       0.032117     0.016694   \n",
              " 1  0.014586  0.021838  0.024044   0.08092  ...       0.032393     0.016832   \n",
              " 2  0.014674  0.021838  0.024044   0.08092  ...       0.032481     0.016876   \n",
              " 3  0.015676  0.021838  0.024044   0.08092  ...       0.033483     0.017377   \n",
              " 4  0.012701  0.022181  0.024044   0.08092  ...       0.030508     0.015949   \n",
              " \n",
              "    exp_ret_conservative   prob_up  tail_spread  signal_exp  signal_prob  \\\n",
              " 0              0.011238  0.860887     0.185918           1            1   \n",
              " 1              0.011404  0.861051     0.185918           1            1   \n",
              " 2              0.011456  0.861102     0.185918           1            1   \n",
              " 3              0.012057  0.861683     0.185918           1            1   \n",
              " 4              0.010306  0.859905     0.185918           1            1   \n",
              " \n",
              "    signal_quantile  pred_q50_orig  signal_prob_exp_q50  \n",
              " 0                0      -0.000115                    0  \n",
              " 1                0      -0.000115                    0  \n",
              " 2                0      -0.000530                    0  \n",
              " 3                0      -0.000530                    0  \n",
              " 4                0      -0.000530                    0  \n",
              " \n",
              " [5 rows x 22 columns],\n",
              "             timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2024-11-04 21:00:00  0.130250 -0.077052 -0.053408  0.074706  0.074706   \n",
              " 1 2024-11-04 22:00:00  0.119305 -0.077052 -0.053408  0.067782  0.067782   \n",
              " 2 2024-11-04 23:00:00  0.122509 -0.077052 -0.053408  0.073519  0.073519   \n",
              " 3 2024-11-05 00:00:00  0.122598 -0.077052 -0.053408  0.064583  0.064583   \n",
              " 4 2024-11-05 01:00:00  0.120685 -0.077052 -0.053408  0.061872  0.061872   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  ...  max_cross_gap  exp_ret_avg  \\\n",
              " 0  0.074706  0.074706  0.095152  0.095152  ...       0.094295     0.061885   \n",
              " 1  0.067782  0.072415  0.093625  0.093625  ...       0.087371     0.057832   \n",
              " 2  0.073519  0.073519  0.093625  0.093625  ...       0.093108     0.060893   \n",
              " 3  0.064583  0.064583  0.076849  0.082645  ...       0.084172     0.053054   \n",
              " 4  0.061872  0.061872  0.076849  0.082645  ...       0.081461     0.051224   \n",
              " \n",
              "    exp_ret_conservative   prob_up  tail_spread  signal_exp  signal_prob  \\\n",
              " 0              0.056776  0.879156     0.183904           1            1   \n",
              " 1              0.052240  0.877965     0.183904           1            1   \n",
              " 2              0.055793  0.878961     0.183904           1            1   \n",
              " 3              0.048150  0.877368     0.183904           1            1   \n",
              " 4              0.046252  0.876835     0.183904           1            1   \n",
              " \n",
              "    signal_quantile  pred_q50_orig  signal_prob_exp_q50  \n",
              " 0                1       0.020667                    1  \n",
              " 1                1       0.020667                    1  \n",
              " 2                1       0.020667                    1  \n",
              " 3                0       0.015703                    1  \n",
              " 4                0       0.015703                    1  \n",
              " \n",
              " [5 rows x 22 columns],\n",
              "             timestamp    y_true  pred_q05  pred_q10  pred_q15  pred_q25  \\\n",
              " 0 2025-03-22 20:00:00  0.039409 -0.080557 -0.067324 -0.064692 -0.029207   \n",
              " 1 2025-03-22 21:00:00  0.041341 -0.080557 -0.067324 -0.062127 -0.029207   \n",
              " 2 2025-03-22 22:00:00  0.039506 -0.080557 -0.067324 -0.065223 -0.029207   \n",
              " 3 2025-03-22 23:00:00  0.039612 -0.080557 -0.067324 -0.067320 -0.029207   \n",
              " 4 2025-03-23 00:00:00  0.039097 -0.080557 -0.067324 -0.063027 -0.029207   \n",
              " \n",
              "    pred_q50  pred_q75  pred_q85  pred_q90  ...  max_cross_gap  exp_ret_avg  \\\n",
              " 0  0.000369  0.026686  0.053891  0.084143  ...            0.0     0.001654   \n",
              " 1  0.000369  0.026686  0.052900  0.084143  ...            0.0     0.001772   \n",
              " 2  0.000369  0.026686  0.055688  0.084143  ...            0.0     0.001749   \n",
              " 3  0.000369  0.026686  0.055688  0.084143  ...            0.0     0.001592   \n",
              " 4 -0.000289  0.025748  0.061642  0.084143  ...            0.0     0.002032   \n",
              " \n",
              "    exp_ret_conservative   prob_up  tail_spread  signal_exp  signal_prob  \\\n",
              " 0             -0.010188  0.503116     0.187409           1            0   \n",
              " 1             -0.009981  0.503116     0.187409           1            0   \n",
              " 2             -0.010151  0.503116     0.187409           1            0   \n",
              " 3             -0.010361  0.503116     0.187409           1            0   \n",
              " 4             -0.009892  0.497228     0.187409           1            0   \n",
              " \n",
              "    signal_quantile  pred_q50_orig  signal_prob_exp_q50  \n",
              " 0                1       0.000369                    0  \n",
              " 1                1       0.000369                    0  \n",
              " 2                1       0.000369                    0  \n",
              " 3                1       0.000369                    0  \n",
              " 4                1      -0.000289                    0  \n",
              " \n",
              " [5 rows x 22 columns])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def add_prob_exp_q50_indicator(\n",
        "    signals_df: pd.DataFrame,\n",
        "    original_df: pd.DataFrame,\n",
        "    tau_long: float = 0.55,\n",
        "    tau_short: float = 0.45,\n",
        "    exp_col: str = \"exp_ret_avg\",\n",
        "    out_col: str = \"signal_prob_exp_q50\",\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Add a -1/0/1 indicator based on prob_up, expected return, and original pred_q50.\n",
        "\n",
        "    Long rule:  prob_up >= tau_long and exp_col > pred_q50_orig > 0  →  +1\n",
        "    Short rule: prob_up <  tau_short and exp_col < pred_q50_orig < 0  →  -1\n",
        "    Else 0.\n",
        "    \"\"\"\n",
        "    if \"prob_up\" not in signals_df.columns:\n",
        "        raise KeyError(\"signals_df must contain 'prob_up'\")\n",
        "    if exp_col not in signals_df.columns:\n",
        "        raise KeyError(f\"signals_df must contain '{exp_col}'\")\n",
        "    if \"pred_q50\" not in original_df.columns:\n",
        "        raise KeyError(\"original_df must contain 'pred_q50'\")\n",
        "\n",
        "    merged = signals_df.copy()\n",
        "    if \"timestamp\" in signals_df.columns and \"timestamp\" in original_df.columns:\n",
        "        q50_map = original_df[[\"timestamp\", \"pred_q50\"]].rename(columns={\"pred_q50\": \"pred_q50_orig\"})\n",
        "        merged = merged.merge(q50_map, on=\"timestamp\", how=\"left\")\n",
        "    else:\n",
        "        # Fallback: align by row order\n",
        "        merged[\"pred_q50_orig\"] = original_df[\"pred_q50\"].values[: len(merged)]\n",
        "\n",
        "    prob = pd.to_numeric(merged[\"prob_up\"], errors=\"coerce\")\n",
        "    expv = pd.to_numeric(merged[exp_col], errors=\"coerce\")\n",
        "    q50o = pd.to_numeric(merged[\"pred_q50_orig\"], errors=\"coerce\")\n",
        "\n",
        "    cond_long = (prob >= float(tau_long)) & (expv > q50o) & (q50o > 0.0)\n",
        "    cond_short = (prob < float(tau_short)) & (expv < q50o) & (q50o < 0.0)\n",
        "    indicator = np.where(cond_long, 1, np.where(cond_short, -1, 0)).astype(int)\n",
        "\n",
        "    out = merged.copy()\n",
        "    out[out_col] = indicator\n",
        "    return out\n",
        "\n",
        "# Add indicator to each split\n",
        "train_ind = add_prob_exp_q50_indicator(train_df, orig_train_df, tau_long=0.55, tau_short=0.48, exp_col=\"exp_ret_avg\", out_col=\"signal_prob_exp_q50\")\n",
        "val_ind = add_prob_exp_q50_indicator(val_df, orig_val_df, tau_long=0.55, tau_short=0.48, exp_col=\"exp_ret_avg\", out_col=\"signal_prob_exp_q50\")\n",
        "test_ind = add_prob_exp_q50_indicator(test_df, orig_test_df, tau_long=0.55, tau_short=0.48, exp_col=\"exp_ret_avg\", out_col=\"signal_prob_exp_q50\")\n",
        "\n",
        "print(\"Indicator value counts (train/val/test):\")\n",
        "print(train_ind[\"signal_prob_exp_q50\"].value_counts().sort_index())\n",
        "print(val_ind[\"signal_prob_exp_q50\"].value_counts().sort_index())\n",
        "print(test_ind[\"signal_prob_exp_q50\"].value_counts().sort_index())\n",
        "\n",
        "train_ind.head(), val_ind.head(), test_ind.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: trades=8013, win_rate=0.939, cumulative_pct=40776.119003\n",
            "val: trades=170, win_rate=0.718, cumulative_pct=690.828619\n",
            "test: trades=31, win_rate=0.742, cumulative_pct=149.336846\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(timestamp\n",
              " 2024-11-04 16:00:00    40725.304104\n",
              " 2024-11-04 17:00:00    40737.687552\n",
              " 2024-11-04 18:00:00    40750.550515\n",
              " 2024-11-04 19:00:00    40762.857117\n",
              " 2024-11-04 20:00:00    40776.119003\n",
              " dtype: float64,\n",
              " timestamp\n",
              " 2025-03-22 15:00:00    690.828619\n",
              " 2025-03-22 16:00:00    690.828619\n",
              " 2025-03-22 17:00:00    690.828619\n",
              " 2025-03-22 18:00:00    690.828619\n",
              " 2025-03-22 19:00:00    690.828619\n",
              " dtype: float64,\n",
              " timestamp\n",
              " 2025-08-07 16:00:00    149.336846\n",
              " 2025-08-07 17:00:00    149.336846\n",
              " 2025-08-07 18:00:00    149.336846\n",
              " 2025-08-07 19:00:00    149.336846\n",
              " 2025-08-07 20:00:00    149.336846\n",
              " dtype: float64)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_indicator(df: pd.DataFrame, name: str):\n",
        "    # Signals: -1/0/1\n",
        "    s = pd.to_numeric(df['signal_prob_exp_q50'], errors='coerce').fillna(0).astype(int)\n",
        "    # Log returns\n",
        "    y_log = pd.to_numeric(df['y_true'], errors='coerce')\n",
        "\n",
        "    trades = (s != 0)\n",
        "    num_trades = int(trades.sum())\n",
        "    wins = (np.sign(y_log[trades]) == s[trades]) & (np.sign(y_log[trades]) != 0)\n",
        "    win_rate = float(wins.mean()) if num_trades > 0 else float('nan')\n",
        "\n",
        "    # Per-trade simple return (not compounded): long = exp(y)-1, short = -(exp(y)-1)\n",
        "    simple_ret = np.exp(y_log) - 1.0\n",
        "    pnl_pct = s * simple_ret*100  # per-row percentage PnL (not log)\n",
        "\n",
        "    # Cumulative (non-compounded) percentage PnL\n",
        "    cumulative_pct = float(pnl_pct.sum())\n",
        "\n",
        "    if 'timestamp' in df.columns:\n",
        "        ts = pd.to_datetime(df['timestamp'], errors='coerce', utc=True).dt.tz_convert('UTC').dt.tz_localize(None)\n",
        "        cum_series = pnl_pct.set_axis(ts).cumsum()\n",
        "    else:\n",
        "        cum_series = pnl_pct.cumsum()\n",
        "\n",
        "    print(f\"{name}: trades={num_trades}, win_rate={win_rate:.3f}, cumulative_pct={cumulative_pct:.6f}\")\n",
        "    return cum_series\n",
        "\n",
        "cum_train = evaluate_indicator(train_ind, 'train')\n",
        "cum_val = evaluate_indicator(val_ind, 'val')\n",
        "cum_test = evaluate_indicator(test_ind, 'test')\n",
        "\n",
        "cum_train.tail(), cum_val.tail(), cum_test.tail()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: Ridge[qs] (alpha=10.0) RMSE=0.00959049, Corr=0.9247\n",
            "val: Ridge[qs] (alpha=10.0) RMSE=0.0281981, Corr=0.1419\n",
            "test: Ridge[qs] (alpha=10.0) RMSE=0.0217679, Corr=0.0150\n"
          ]
        }
      ],
      "source": [
        "# Ridge regression on quantile vector to predict y_true (train+val fit)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select quantile columns like pred_q05, pred_q10, ..., pred_q95\n",
        "qcols = [c for c in train_df.columns if re.fullmatch(r'pred_q\\d{2}', str(c))]\n",
        "qcols = sorted(qcols, key=lambda c: int(c[-2:]))\n",
        "if not qcols:\n",
        "    raise ValueError(\"No quantile columns found in train_df matching 'pred_q##'.\")\n",
        "\n",
        "trv_X = pd.concat([train_df[qcols], val_df[qcols]], ignore_index=True).apply(pd.to_numeric, errors='coerce')\n",
        "trv_y = pd.concat([train_df['y_true'], val_df['y_true']], ignore_index=True).apply(pd.to_numeric, errors='coerce')\n",
        "mask = trv_X.notna().all(axis=1) & trv_y.notna()\n",
        "trv_X = trv_X.loc[mask].to_numpy(dtype=float)\n",
        "trv_y = trv_y.loc[mask].to_numpy(dtype=float)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "trv_Xs = scaler.fit_transform(trv_X)\n",
        "\n",
        "# Simple time series CV to choose alpha\n",
        "alphas = [0.0, 0.01, 0.1, 1.0, 5.0, 10.0]\n",
        "cv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "best_alpha, best_rmse = None, float('inf')\n",
        "for a in alphas:\n",
        "    rmses = []\n",
        "    for tr_idx, va_idx in cv.split(trv_Xs):\n",
        "        Xtr, Xva = trv_Xs[tr_idx], trv_Xs[va_idx]\n",
        "        ytr, yva = trv_y[tr_idx], trv_y[va_idx]\n",
        "        model = Ridge(alpha=a, fit_intercept=True, random_state=42)\n",
        "        model.fit(Xtr, ytr)\n",
        "        yhat = model.predict(Xva)\n",
        "        rmses.append(np.sqrt(np.mean((yhat - yva) ** 2)))\n",
        "    avg_rmse = float(np.mean(rmses))\n",
        "    if avg_rmse < best_rmse:\n",
        "        best_rmse, best_alpha = avg_rmse, a\n",
        "\n",
        "model = Ridge(alpha=best_alpha, fit_intercept=True, random_state=42)\n",
        "model.fit(trv_Xs, trv_y)\n",
        "\n",
        "# Evaluate on splits\n",
        "for name, d in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    X = d[qcols].apply(pd.to_numeric, errors='coerce')\n",
        "    m = X.notna().all(axis=1) & pd.to_numeric(d['y_true'], errors='coerce').notna()\n",
        "    Xs = scaler.transform(X.loc[m].to_numpy(dtype=float))\n",
        "    y = pd.to_numeric(d['y_true'], errors='coerce').loc[m].to_numpy(dtype=float)\n",
        "    yhat = model.predict(Xs)\n",
        "    rmse_val = float(np.sqrt(np.mean((yhat - y) ** 2))) if len(y) else float('nan')\n",
        "    corr_val = float(np.corrcoef(yhat, y)[0,1]) if len(y) else float('nan')\n",
        "    print(f\"{name}: Ridge[qs] (alpha={best_alpha}) RMSE={rmse_val:.6g}, Corr={corr_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics (RMSE for exp_ret_avg, Brier for prob_up):\n",
            "train: RMSE(exp_ret_avg,y_true)=0.0207627, Brier(prob_up)=0.195347\n",
            "val: RMSE(exp_ret_avg,y_true)=0.0271975, Brier(prob_up)=0.248301\n",
            "test: RMSE(exp_ret_avg,y_true)=0.0187889, Brier(prob_up)=0.251163\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(a, b):\n",
        "    a = np.asarray(a, dtype=float)\n",
        "    b = np.asarray(b, dtype=float)\n",
        "    return float(np.sqrt(np.mean((a - b) ** 2)))\n",
        "\n",
        "def brier(p, ybin):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    yb = np.asarray(ybin, dtype=float)\n",
        "    return float(np.mean((p - yb) ** 2))\n",
        "\n",
        "\n",
        "def eval_split(df: pd.DataFrame, name: str):\n",
        "    y = pd.to_numeric(df['y_true'], errors='coerce')\n",
        "    exp = pd.to_numeric(df['exp_ret_avg'], errors='coerce')\n",
        "    prob = pd.to_numeric(df['prob_up'], errors='coerce')\n",
        "\n",
        "    # RMSE for expected return vs true\n",
        "    m = y.notna() & exp.notna()\n",
        "    rmse_val = rmse(exp[m], y[m]) if m.any() else float('nan')\n",
        "\n",
        "    # Brier score for probability of positive\n",
        "    ybin = (y > 0).astype(int)\n",
        "    m2 = ybin.notna() & prob.notna()\n",
        "    brier_val = brier(prob[m2], ybin[m2]) if m2.any() else float('nan')\n",
        "\n",
        "    print(f\"{name}: RMSE(exp_ret_avg,y_true)={rmse_val:.6g}, Brier(prob_up)={brier_val:.6g}\")\n",
        "    return rmse_val, brier_val\n",
        "\n",
        "print(\"Metrics (RMSE for exp_ret_avg, Brier for prob_up):\")\n",
        "rmse_train, brier_train = eval_split(train_ind, 'train')\n",
        "rmse_val, brier_val = eval_split(val_ind, 'val')\n",
        "rmse_test, brier_test = eval_split(test_ind, 'test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick checks:\n",
            "train:\n",
            "  RMSE(avg)=0.0209008, NRMSE(avg)=0.839464, RMSE(cons)=0.0220408, NRMSE(cons)=0.885251\n",
            "  Corr(avg,y)=0.8385, R2(avg)=0.7031\n",
            "  Brier=0.23619, BaseBrier=0.24943, BSS=0.0531, p_hat=0.524\n",
            "  ECE=0.0909\n",
            "  prob_up summary: mean=0.518, std=0.037, median=0.518, p10=0.491, p90=0.553\n",
            "val:\n",
            "  RMSE(avg)=0.0270382, NRMSE(avg)=0.986567, RMSE(cons)=0.0277797, NRMSE(cons)=1.01362\n",
            "  Corr(avg,y)=0.2047, R2(avg)=0.0419\n",
            "  Brier=0.249018, BaseBrier=0.249879, BSS=0.0034, p_hat=0.511\n",
            "  ECE=0.0127\n",
            "  prob_up summary: mean=0.512, std=0.016, median=0.509, p10=0.493, p90=0.536\n",
            "test:\n",
            "  RMSE(avg)=0.0188572, NRMSE(avg)=1.00241, RMSE(cons)=0.020022, NRMSE(cons)=1.06432\n",
            "  Corr(avg,y)=0.0525, R2(avg)=0.0028\n",
            "  Brier=0.248689, BaseBrier=0.247644, BSS=-0.0042, p_hat=0.549\n",
            "  ECE=0.0337\n",
            "  prob_up summary: mean=0.515, std=0.011, median=0.514, p10=0.502, p90=0.527\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(          bin  count    mean_p  emp_rate\n",
              " 0  (0.4, 0.5]    628  0.491131  0.455414\n",
              " 1  (0.5, 0.6]   2697  0.516560  0.523915,\n",
              "           bin  count    mean_p  emp_rate\n",
              " 0  (0.4, 0.5]    122  0.497169  0.704918\n",
              " 1  (0.5, 0.6]   3205  0.515496  0.542590)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Quick checks: normalized errors, base-rate Brier/BSS, reliability/ECE, correlations\n",
        "\n",
        "def rmse(a, b):\n",
        "    a = np.asarray(a, dtype=float)\n",
        "    b = np.asarray(b, dtype=float)\n",
        "    return float(np.sqrt(np.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def corr_r2(a, b):\n",
        "    a = pd.to_numeric(a, errors='coerce')\n",
        "    b = pd.to_numeric(b, errors='coerce')\n",
        "    m = a.notna() & b.notna()\n",
        "    if not m.any():\n",
        "        return float('nan'), float('nan')\n",
        "    r = float(np.corrcoef(a[m], b[m])[0, 1])\n",
        "    return r, r * r\n",
        "\n",
        "\n",
        "def brier_score(p, ybin):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    yb = np.asarray(ybin, dtype=float)\n",
        "    return float(np.mean((p - yb) ** 2))\n",
        "\n",
        "\n",
        "def base_rate_brier(ybin):\n",
        "    p_hat = float(np.mean(ybin))\n",
        "    return p_hat * (1.0 - p_hat), p_hat\n",
        "\n",
        "\n",
        "def reliability_bins(prob, ybin, num_bins=10):\n",
        "    df = pd.DataFrame({'p': pd.to_numeric(prob, errors='coerce'), 'y': pd.to_numeric(ybin, errors='coerce')})\n",
        "    df = df.dropna()\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(), float('nan')\n",
        "    edges = np.linspace(0.0, 1.0, num_bins + 1)\n",
        "    df['bin'] = pd.cut(df['p'], bins=edges, include_lowest=True, right=True)\n",
        "    grp = df.groupby('bin', observed=True)\n",
        "    tab = grp.agg(count=('y', 'size'), mean_p=('p', 'mean'), emp_rate=('y', 'mean')).reset_index()\n",
        "    N = float(tab['count'].sum())\n",
        "    tab = tab[tab['count'] > 0]\n",
        "    ece = float(np.sum((tab['count'] / N) * np.abs(tab['mean_p'] - tab['emp_rate']))) if N > 0 else float('nan')\n",
        "    return tab, ece\n",
        "\n",
        "\n",
        "def split_quick_checks(df: pd.DataFrame, name: str):\n",
        "    y = pd.to_numeric(df['y_true'], errors='coerce')\n",
        "    exp_avg = pd.to_numeric(df['exp_ret_avg'], errors='coerce')\n",
        "    exp_con = pd.to_numeric(df['exp_ret_conservative'], errors='coerce')\n",
        "    prob = pd.to_numeric(df['prob_up'], errors='coerce')\n",
        "    ybin = (y > 0).astype(int)\n",
        "\n",
        "    # RMSE and normalized RMSE\n",
        "    mask_avg = y.notna() & exp_avg.notna()\n",
        "    sigma_y = float(y[mask_avg].std(ddof=0)) if mask_avg.any() else float('nan')\n",
        "    rmse_avg = rmse(exp_avg[mask_avg], y[mask_avg]) if mask_avg.any() else float('nan')\n",
        "    nrmse_avg = (rmse_avg / sigma_y) if (sigma_y and sigma_y > 0) else float('nan')\n",
        "\n",
        "    mask_con = y.notna() & exp_con.notna()\n",
        "    rmse_con = rmse(exp_con[mask_con], y[mask_con]) if mask_con.any() else float('nan')\n",
        "    nrmse_con = (rmse_con / sigma_y) if (sigma_y and sigma_y > 0) else float('nan')\n",
        "\n",
        "    # Correlation / R^2\n",
        "    r_avg, r2_avg = corr_r2(exp_avg, y)\n",
        "\n",
        "    # Brier, base-rate Brier, BSS\n",
        "    mask_prob = prob.notna() & ybin.notna()\n",
        "    brier = brier_score(prob[mask_prob], ybin[mask_prob]) if mask_prob.any() else float('nan')\n",
        "    brier_base, p_hat = base_rate_brier(ybin[mask_prob]) if mask_prob.any() else (float('nan'), float('nan'))\n",
        "    bss = (1.0 - brier / brier_base) if (brier_base and brier_base > 0) else float('nan')\n",
        "\n",
        "    # Reliability / ECE\n",
        "    rel, ece = reliability_bins(prob[mask_prob], ybin[mask_prob], num_bins=10) if mask_prob.any() else (pd.DataFrame(), float('nan'))\n",
        "\n",
        "    # Prob summary\n",
        "    prob_desc = prob.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])\n",
        "\n",
        "    print(f\"{name}:\\n\"\n",
        "          f\"  RMSE(avg)={rmse_avg:.6g}, NRMSE(avg)={nrmse_avg:.6g}, RMSE(cons)={rmse_con:.6g}, NRMSE(cons)={nrmse_con:.6g}\\n\"\n",
        "          f\"  Corr(avg,y)={r_avg:.4f}, R2(avg)={r2_avg:.4f}\\n\"\n",
        "          f\"  Brier={brier:.6g}, BaseBrier={brier_base:.6g}, BSS={bss:.4f}, p_hat={p_hat:.3f}\\n\"\n",
        "          f\"  ECE={ece:.4f}\\n\"\n",
        "          f\"  prob_up summary: mean={prob_desc['mean']:.3f}, std={prob_desc['std']:.3f}, median={prob_desc['50%']:.3f}, p10={prob_desc['10%']:.3f}, p90={prob_desc['90%']:.3f}\")\n",
        "    return {\n",
        "        'rmse_avg': rmse_avg, 'nrmse_avg': nrmse_avg, 'rmse_con': rmse_con, 'nrmse_con': nrmse_con,\n",
        "        'corr_avg': r_avg, 'r2_avg': r2_avg, 'brier': brier, 'brier_base': brier_base, 'bss': bss, 'p_hat': p_hat,\n",
        "        'ece': ece, 'reliability': rel,\n",
        "    }\n",
        "\n",
        "print(\"Quick checks:\")\n",
        "qc_train = split_quick_checks(train_ind, 'train')\n",
        "qc_val = split_quick_checks(val_ind, 'val')\n",
        "qc_test = split_quick_checks(test_ind, 'test')\n",
        "\n",
        "# Show non-empty reliability tables for val/test\n",
        "qc_val['reliability'].head(12), qc_test['reliability'].head(12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: OLS[exp] RMSE=0.0118507, Corr=0.8799 | OLS[exp,tail] RMSE=0.0101492, Corr=0.9160\n",
            "val: OLS[exp] RMSE=0.0279753, Corr=0.1333 | OLS[exp,tail] RMSE=0.0288044, Corr=0.1192\n",
            "test: OLS[exp] RMSE=0.0195037, Corr=0.0989 | OLS[exp,tail] RMSE=0.0224342, Corr=0.0767\n"
          ]
        }
      ],
      "source": [
        "# Post-hoc linear mapping from exp_ret to y_true (train+val fit, test untouched)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Build train+val frame\n",
        "trv = pd.concat([train_df, val_df], ignore_index=True)\n",
        "\n",
        "# Helper: closed-form OLS with intercept for columns in Xcols\n",
        "def fit_ols(df, ycol, xcols):\n",
        "    y = pd.to_numeric(df[ycol], errors='coerce')\n",
        "    X = df[xcols].apply(pd.to_numeric, errors='coerce')\n",
        "    m = y.notna()\n",
        "    for c in xcols:\n",
        "        m &= X[c].notna()\n",
        "    y = y[m].to_numpy(dtype=float)\n",
        "    X = X.loc[m, :].to_numpy(dtype=float)\n",
        "    # Add intercept\n",
        "    Xw = np.column_stack([np.ones(len(X)), X])\n",
        "    beta, *_ = np.linalg.lstsq(Xw, y, rcond=None)\n",
        "    return beta, xcols\n",
        "\n",
        "# Fit two variants on train+val\n",
        "beta1, cols1 = fit_ols(trv, 'y_true', ['exp_ret_avg'])\n",
        "beta2, cols2 = fit_ols(trv, 'y_true', ['exp_ret_avg', 'tail_spread'])\n",
        "\n",
        "# Predict helper\n",
        "def predict_ols(df, beta, xcols):\n",
        "    X = df[xcols].apply(pd.to_numeric, errors='coerce').to_numpy(dtype=float)\n",
        "    Xw = np.column_stack([np.ones(len(X)), X])\n",
        "    return Xw @ beta\n",
        "\n",
        "# Evaluate on splits\n",
        "for name, d in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    y = pd.to_numeric(d['y_true'], errors='coerce').to_numpy(dtype=float)\n",
        "    m1 = np.isfinite(y)\n",
        "    y1 = y[m1]\n",
        "    yhat1 = predict_ols(d.iloc[m1.nonzero()[0]], beta1, cols1)\n",
        "    yhat2 = predict_ols(d.iloc[m1.nonzero()[0]], beta2, cols2)\n",
        "    def _rmse(a,b):\n",
        "        return float(np.sqrt(np.mean((a-b)**2))) if len(a)>0 else float('nan')\n",
        "    def _corr(a,b):\n",
        "        if len(a) == 0:\n",
        "            return float('nan')\n",
        "        c = np.corrcoef(a, b)[0,1]\n",
        "        return float(c)\n",
        "    rm1, r1 = _rmse(y1, yhat1), _corr(y1, yhat1)\n",
        "    rm2, r2 = _rmse(y1, yhat2), _corr(y1, yhat2)\n",
        "    print(f\"{name}: OLS[exp] RMSE={rm1:.6g}, Corr={r1:.4f} | OLS[exp,tail] RMSE={rm2:.6g}, Corr={r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: Ridge[qs] (alpha=10.0) RMSE=0.00959049, Corr=0.9247\n",
            "val: Ridge[qs] (alpha=10.0) RMSE=0.0281981, Corr=0.1419\n",
            "test: Ridge[qs] (alpha=10.0) RMSE=0.0217679, Corr=0.0150\n"
          ]
        }
      ],
      "source": [
        "# Ridge regression on quantile vector to predict y_true (train+val fit)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select quantile columns like pred_q05, pred_q10, ..., pred_q95\n",
        "qcols = [c for c in train_df.columns if re.fullmatch(r'pred_q\\d{2}', str(c))]\n",
        "qcols = sorted(qcols, key=lambda c: int(c[-2:]))\n",
        "if not qcols:\n",
        "    raise ValueError(\"No quantile columns found in train_df matching 'pred_q##'.\")\n",
        "\n",
        "trv_X = pd.concat([train_df[qcols], val_df[qcols]], ignore_index=True).apply(pd.to_numeric, errors='coerce')\n",
        "trv_y = pd.concat([train_df['y_true'], val_df['y_true']], ignore_index=True).apply(pd.to_numeric, errors='coerce')\n",
        "mask = trv_X.notna().all(axis=1) & trv_y.notna()\n",
        "trv_X = trv_X.loc[mask].to_numpy(dtype=float)\n",
        "trv_y = trv_y.loc[mask].to_numpy(dtype=float)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "trv_Xs = scaler.fit_transform(trv_X)\n",
        "\n",
        "# Simple time series CV to choose alpha\n",
        "alphas = [0.0, 0.01, 0.1, 1.0, 5.0, 10.0]\n",
        "cv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "best_alpha, best_rmse = None, float('inf')\n",
        "for a in alphas:\n",
        "    rmses = []\n",
        "    for tr_idx, va_idx in cv.split(trv_Xs):\n",
        "        Xtr, Xva = trv_Xs[tr_idx], trv_Xs[va_idx]\n",
        "        ytr, yva = trv_y[tr_idx], trv_y[va_idx]\n",
        "        model = Ridge(alpha=a, fit_intercept=True, random_state=42)\n",
        "        model.fit(Xtr, ytr)\n",
        "        yhat = model.predict(Xva)\n",
        "        rmses.append(np.sqrt(np.mean((yhat - yva) ** 2)))\n",
        "    avg_rmse = float(np.mean(rmses))\n",
        "    if avg_rmse < best_rmse:\n",
        "        best_rmse, best_alpha = avg_rmse, a\n",
        "\n",
        "model = Ridge(alpha=best_alpha, fit_intercept=True, random_state=42)\n",
        "model.fit(trv_Xs, trv_y)\n",
        "\n",
        "# Evaluate on splits\n",
        "for name, d in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    X = d[qcols].apply(pd.to_numeric, errors='coerce')\n",
        "    m = X.notna().all(axis=1) & pd.to_numeric(d['y_true'], errors='coerce').notna()\n",
        "    Xs = scaler.transform(X.loc[m].to_numpy(dtype=float))\n",
        "    y = pd.to_numeric(d['y_true'], errors='coerce').loc[m].to_numpy(dtype=float)\n",
        "    yhat = model.predict(Xs)\n",
        "    rmse_val = float(np.sqrt(np.mean((yhat - y) ** 2))) if len(y) else float('nan')\n",
        "    corr_val = float(np.corrcoef(yhat, y)[0,1]) if len(y) else float('nan')\n",
        "    print(f\"{name}: Ridge[qs] (alpha={best_alpha}) RMSE={rmse_val:.6g}, Corr={corr_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: Isotonic(exp) RMSE=0.0130173, Corr=0.8576\n",
            "val: Isotonic(exp) RMSE=0.0273142, Corr=0.1886\n",
            "test: Isotonic(exp) RMSE=0.0202509, Corr=0.0481\n"
          ]
        }
      ],
      "source": [
        "# Isotonic regression mapping y ~ f(exp_ret_avg) (train+val fit)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "# Prepare train+val\n",
        "trv_exp = pd.to_numeric(trv['exp_ret_avg'], errors='coerce')\n",
        "trv_y = pd.to_numeric(trv['y_true'], errors='coerce')\n",
        "mask = trv_exp.notna() & trv_y.notna()\n",
        "X_trv = trv_exp.loc[mask].to_numpy(dtype=float)\n",
        "y_trv = trv_y.loc[mask].to_numpy(dtype=float)\n",
        "\n",
        "iso = IsotonicRegression(y_min=None, y_max=None, increasing=True, out_of_bounds='clip')\n",
        "iso.fit(X_trv, y_trv)\n",
        "\n",
        "# Evaluate\n",
        "for name, d in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    x = pd.to_numeric(d['exp_ret_avg'], errors='coerce')\n",
        "    y = pd.to_numeric(d['y_true'], errors='coerce')\n",
        "    m = x.notna() & y.notna()\n",
        "    x1 = x.loc[m].to_numpy(dtype=float)\n",
        "    y1 = y.loc[m].to_numpy(dtype=float)\n",
        "    yhat = iso.predict(x1)\n",
        "    rmse_val = float(np.sqrt(np.mean((yhat - y1) ** 2))) if len(y1) else float('nan')\n",
        "    corr_val = float(np.corrcoef(yhat, y1)[0,1]) if len(y1) else float('nan')\n",
        "    print(f\"{name}: Isotonic(exp) RMSE={rmse_val:.6g}, Corr={corr_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: added exp_ret_cal, prob_up_cal; shapes now (15521, 22)\n",
            "val: added exp_ret_cal, prob_up_cal; shapes now (3325, 22)\n",
            "test: added exp_ret_cal, prob_up_cal; shapes now (3327, 22)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(   exp_ret_avg  exp_ret_cal   prob_up  prob_up_cal\n",
              " 0     0.003349     0.009203  0.521131     0.652327\n",
              " 1     0.003528     0.009934  0.521131     0.652327\n",
              " 2     0.003232     0.009203  0.521131     0.652327\n",
              " 3     0.003621     0.009934  0.521131     0.652327\n",
              " 4     0.003398     0.009203  0.521131     0.652327,\n",
              "    exp_ret_avg  exp_ret_cal   prob_up  prob_up_cal\n",
              " 0    -0.001214    -0.006704  0.535163     0.781407\n",
              " 1    -0.001400    -0.007575  0.535163     0.781407\n",
              " 2    -0.001160    -0.006704  0.535163     0.781407\n",
              " 3    -0.001399    -0.007575  0.526237     0.693793\n",
              " 4    -0.001282    -0.006704  0.526237     0.693793,\n",
              "    exp_ret_avg  exp_ret_cal   prob_up  prob_up_cal\n",
              " 0    -0.001586    -0.008531  0.514295     0.479282\n",
              " 1    -0.001865    -0.009556  0.514295     0.479282\n",
              " 2    -0.002095    -0.010184  0.514295     0.479282\n",
              " 3    -0.002016    -0.010184  0.514295     0.479282\n",
              " 4    -0.002201    -0.012830  0.514295     0.479282)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calibrate exp_ret_avg and prob_up via isotonic (fit on train+val), then apply to all splits\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "# Prepare train+val data\n",
        "trv_all = pd.concat([train_df, val_df], ignore_index=True)\n",
        "\n",
        "# Calibrator for expected return: exp_ret_avg -> y_true\n",
        "x_exp = pd.to_numeric(trv_all['exp_ret_avg'], errors='coerce')\n",
        "y_exp = pd.to_numeric(trv_all['y_true'], errors='coerce')\n",
        "mask_exp = x_exp.notna() & y_exp.notna()\n",
        "iso_exp = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "iso_exp.fit(x_exp.loc[mask_exp].to_numpy(dtype=float), y_exp.loc[mask_exp].to_numpy(dtype=float))\n",
        "\n",
        "# Calibrator for probability: prob_up -> 1[y_true>0]\n",
        "p_prob = pd.to_numeric(trv_all['prob_up'], errors='coerce')\n",
        "y_bin = (pd.to_numeric(trv_all['y_true'], errors='coerce') > 0).astype(float)\n",
        "mask_prob = p_prob.notna() & y_bin.notna()\n",
        "iso_prob = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "iso_prob.fit(p_prob.loc[mask_prob].to_numpy(dtype=float), y_bin.loc[mask_prob].to_numpy(dtype=float))\n",
        "\n",
        "# Apply to splits\n",
        "for name, d in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    d['exp_ret_cal'] = iso_exp.predict(pd.to_numeric(d['exp_ret_avg'], errors='coerce').to_numpy(dtype=float))\n",
        "    d['prob_up_cal'] = iso_prob.predict(pd.to_numeric(d['prob_up'], errors='coerce').to_numpy(dtype=float))\n",
        "    print(f\"{name}: added exp_ret_cal, prob_up_cal; shapes now {d.shape}\")\n",
        "\n",
        "# Preview\n",
        "train_df[['exp_ret_avg','exp_ret_cal','prob_up','prob_up_cal']].head(), \\\n",
        "val_df[['exp_ret_avg','exp_ret_cal','prob_up','prob_up_cal']].head(), \\\n",
        "test_df[['exp_ret_avg','exp_ret_cal','prob_up','prob_up_cal']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calibrated indicator value counts (train/val/test):\n",
            "signal_prob_exp_q50\n",
            "-1    1839\n",
            " 0    7865\n",
            " 1    5817\n",
            "Name: count, dtype: int64\n",
            "signal_prob_exp_q50\n",
            "-1     369\n",
            " 0    2378\n",
            " 1     578\n",
            "Name: count, dtype: int64\n",
            "signal_prob_exp_q50\n",
            "-1      58\n",
            " 0    2596\n",
            " 1     673\n",
            "Name: count, dtype: int64\n",
            "train_cal: trades=7656, win_rate=0.867, cumulative_return=150.599633\n",
            "val_cal: trades=947, win_rate=0.582, cumulative_return=4.121872\n",
            "test_cal: trades=731, win_rate=0.549, cumulative_return=2.070407\n",
            "train_cal: 25385725917155634173395410323406757775886159554795107967539500548096.00%\n",
            "val_cal:   6067.46%\n",
            "test_cal:  692.80%\n"
          ]
        }
      ],
      "source": [
        "# Apply indicator/evaluation on calibrated expected return and probability\n",
        "import pandas as pd\n",
        "\n",
        "# Helper to use calibrated columns with existing indicator function\n",
        "def add_indicator_with_cal(signals_df: pd.DataFrame, original_df: pd.DataFrame,\n",
        "                           tau_long: float = 0.51, tau_short: float = 0.49,\n",
        "                           exp_col: str = 'exp_ret_cal', prob_col: str = 'prob_up_cal',\n",
        "                           out_col: str = 'signal_prob_exp_q50') -> pd.DataFrame:\n",
        "    tmp = signals_df.copy()\n",
        "    tmp['exp_ret_avg'] = pd.to_numeric(tmp[exp_col], errors='coerce')\n",
        "    tmp['prob_up'] = pd.to_numeric(tmp[prob_col], errors='coerce')\n",
        "    return add_prob_exp_q50_indicator(tmp, original_df, tau_long=tau_long, tau_short=tau_short,\n",
        "                                      exp_col='exp_ret_avg', out_col=out_col)\n",
        "\n",
        "# Build calibrated indicators\n",
        "train_ind_cal = add_indicator_with_cal(train_df, orig_train_df)\n",
        "val_ind_cal = add_indicator_with_cal(val_df, orig_val_df)\n",
        "test_ind_cal = add_indicator_with_cal(test_df, orig_test_df)\n",
        "\n",
        "print(\"Calibrated indicator value counts (train/val/test):\")\n",
        "print(train_ind_cal['signal_prob_exp_q50'].value_counts().sort_index())\n",
        "print(val_ind_cal['signal_prob_exp_q50'].value_counts().sort_index())\n",
        "print(test_ind_cal['signal_prob_exp_q50'].value_counts().sort_index())\n",
        "\n",
        "# Evaluate calibrated indicators\n",
        "cum_train_cal = evaluate_indicator(train_ind_cal, 'train_cal')\n",
        "cum_val_cal = evaluate_indicator(val_ind_cal, 'val_cal')\n",
        "cum_test_cal = evaluate_indicator(test_ind_cal, 'test_cal')\n",
        "\n",
        "print(f\"train_cal: {pct_from_cum_log(cum_train_cal):.2f}%\")\n",
        "print(f\"val_cal:   {pct_from_cum_log(cum_val_cal):.2f}%\")\n",
        "print(f\"test_cal:  {pct_from_cum_log(cum_test_cal):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.pct_from_cum_log(cum_series)>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pct_from_cum_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-quantile residual shifts (train+val):\n",
            "  pred_q05: shift=+0.001509\n",
            "  pred_q10: shift=-0.000529\n",
            "  pred_q15: shift=-0.000030\n",
            "  pred_q25: shift=+0.000630\n",
            "  pred_q50: shift=-0.000044\n",
            "  pred_q75: shift=-0.000278\n",
            "  pred_q85: shift=-0.001768\n",
            "  pred_q90: shift=-0.001677\n",
            "  pred_q95: shift=-0.002197\n",
            "train: calibrated prob_up Brier=0.191965 (cols added: 9 qcal + prob_up_qcal)\n",
            "val: calibrated prob_up Brier=0.248301 (cols added: 9 qcal + prob_up_qcal)\n",
            "test: calibrated prob_up Brier=0.25132 (cols added: 9 qcal + prob_up_qcal)\n"
          ]
        }
      ],
      "source": [
        "# Per-quantile calibration (CQR-style residual shifts), then recompute prob_up\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from strategies.quantile_signals import estimate_prob_up\n",
        "\n",
        "# Identify quantile columns\n",
        "qcols = sorted([c for c in train_df.columns if re.fullmatch(r'pred_q\\d{2}', str(c))], key=lambda c: int(c[-2:]))\n",
        "if not qcols:\n",
        "    raise ValueError(\"No pred_q## columns found.\")\n",
        "\n",
        "# Train+val residual quantiles at each p\n",
        "trv = pd.concat([train_df, val_df], ignore_index=True)\n",
        "shifts: dict[str, float] = {}\n",
        "for c in qcols:\n",
        "    p = int(c[-2:]) / 100.0\n",
        "    y = pd.to_numeric(trv['y_true'], errors='coerce')\n",
        "    q = pd.to_numeric(trv[c], errors='coerce')\n",
        "    resid = (y - q).dropna()\n",
        "    shifts[c] = float(np.quantile(resid.to_numpy(), p)) if len(resid) else 0.0\n",
        "\n",
        "print(\"Per-quantile residual shifts (train+val):\")\n",
        "for c in qcols:\n",
        "    print(f\"  {c}: shift={shifts[c]:+.6f}\")\n",
        "\n",
        "# Apply shifts, enforce monotonicity, recompute prob_up\n",
        "for name, d in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    # Create calibrated quantile columns\n",
        "    qcal = []\n",
        "    for c in qcols:\n",
        "        cc = f\"{c}_qcal\"\n",
        "        d[cc] = pd.to_numeric(d[c], errors='coerce') + shifts[c]\n",
        "        qcal.append(cc)\n",
        "    # Monotone fix\n",
        "    d[qcal] = d[qcal].cummax(axis=1)\n",
        "    # Recompute prob_up from calibrated curve\n",
        "    d['prob_up_qcal'] = d[qcal].apply(lambda row: estimate_prob_up(row, qcols=qcal), axis=1)\n",
        "    # Report Brier score for calibrated prob\n",
        "    ybin = (pd.to_numeric(d['y_true'], errors='coerce') > 0).astype(float)\n",
        "    m = ybin.notna() & d['prob_up_qcal'].notna()\n",
        "    brier = float(np.mean((d.loc[m, 'prob_up_qcal'].to_numpy() - ybin.loc[m].to_numpy())**2)) if m.any() else float('nan')\n",
        "    print(f\"{name}: calibrated prob_up Brier={brier:.6g} (cols added: {len(qcal)} qcal + prob_up_qcal)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined (exp_ret_cal + prob_up_qcal) indicator counts (train/val/test):\n",
            "signal_prob_exp_q50_cal\n",
            "-1    5368\n",
            " 0    5952\n",
            " 1    4201\n",
            "Name: count, dtype: int64\n",
            "signal_prob_exp_q50_cal\n",
            "-1     192\n",
            " 0    2732\n",
            " 1     401\n",
            "Name: count, dtype: int64\n",
            "signal_prob_exp_q50_cal\n",
            "-1     105\n",
            " 0    3171\n",
            " 1      51\n",
            "Name: count, dtype: int64\n",
            "train_combo: trades=9569, win_rate=0.945, cumulative_return=208.857665\n",
            "val_combo: trades=593, win_rate=0.578, cumulative_return=4.179296\n",
            "test_combo: trades=156, win_rate=0.551, cumulative_return=0.397202\n",
            "train_combo: 507845077095603046846669933891975786539291390679672293304488703376732358512808572299924996096.00%\n",
            "val_combo:   6431.98%\n",
            "test_combo:  48.77%\n"
          ]
        }
      ],
      "source": [
        "# Combine isotonic-calibrated exp_ret with per-quantile-calibrated prob_up and evaluate\n",
        "# Uses exp_ret_cal + prob_up_qcal\n",
        "\n",
        "# Build combined indicators\n",
        "train_ind_combo = add_indicator_with_cal(train_df, orig_train_df, exp_col='exp_ret_cal', prob_col='prob_up_qcal', out_col='signal_prob_exp_q50_cal')\n",
        "val_ind_combo = add_indicator_with_cal(val_df, orig_val_df, exp_col='exp_ret_cal', prob_col='prob_up_qcal', out_col='signal_prob_exp_q50_cal')\n",
        "test_ind_combo = add_indicator_with_cal(test_df, orig_test_df, exp_col='exp_ret_cal', prob_col='prob_up_qcal', out_col='signal_prob_exp_q50_cal', tau_long=0.55, tau_short=0.47)\n",
        "\n",
        "print(\"Combined (exp_ret_cal + prob_up_qcal) indicator counts (train/val/test):\")\n",
        "print(train_ind_combo['signal_prob_exp_q50_cal'].value_counts().sort_index())\n",
        "print(val_ind_combo['signal_prob_exp_q50_cal'].value_counts().sort_index())\n",
        "print(test_ind_combo['signal_prob_exp_q50_cal'].value_counts().sort_index())\n",
        "\n",
        "# Evaluate\n",
        "cum_train_combo = evaluate_indicator(train_ind_combo.rename(columns={'signal_prob_exp_q50_cal': 'signal_prob_exp_q50'}), 'train_combo')\n",
        "cum_val_combo = evaluate_indicator(val_ind_combo.rename(columns={'signal_prob_exp_q50_cal': 'signal_prob_exp_q50'}), 'val_combo')\n",
        "cum_test_combo = evaluate_indicator(test_ind_combo.rename(columns={'signal_prob_exp_q50_cal': 'signal_prob_exp_q50'}), 'test_combo')\n",
        "\n",
        "print(f\"train_combo: {pct_from_cum_log(cum_train_combo):.2f}%\")\n",
        "print(f\"val_combo:   {pct_from_cum_log(cum_val_combo):.2f}%\")\n",
        "print(f\"test_combo:  {pct_from_cum_log(cum_test_combo):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calibrated probability diagnostics (orig vs isotonic vs per-quantile):\n",
            "train orig: Brier=0.195347, Base=0.24943, BSS=0.2168, ECE=0.2722, mean=0.492, std=0.091, med=0.521, p10=0.351, p90=0.585, p_hat=0.524\n",
            "       bin  count   mean_p  emp_rate\n",
            "(0.2, 0.3]    690 0.263354  0.011594\n",
            "(0.3, 0.4]   2130 0.357523  0.021596\n",
            "(0.4, 0.5]   3455 0.452548  0.170767\n",
            "(0.5, 0.6]   8555 0.549907  0.795675\n",
            "(0.6, 0.7]    691 0.608626  0.984081\n",
            "train iso: Brier=0.106623, Base=0.24943, BSS=0.5725, ECE=0.0289, mean=0.519, std=0.361, med=0.558, p10=0.026, p90=0.972, p_hat=0.524\n",
            "          bin  count   mean_p  emp_rate\n",
            "(-0.001, 0.1]   3811 0.032168  0.031750\n",
            "   (0.1, 0.2]   1083 0.154273  0.135734\n",
            "   (0.2, 0.3]    323 0.262217  0.201238\n",
            "   (0.3, 0.4]    375 0.337900  0.264000\n",
            "   (0.4, 0.5]   1560 0.440873  0.364103\n",
            "   (0.5, 0.6]   1152 0.561535  0.591146\n",
            "   (0.6, 0.7]    346 0.620469  0.658960\n",
            "   (0.7, 0.8]   1220 0.741029  0.780328\n",
            "   (0.8, 0.9]   2309 0.832284  0.881334\n",
            "   (0.9, 1.0]   3342 0.952635  0.967983\n",
            "train qcal: Brier=0.191965, Base=0.24943, BSS=0.2304, ECE=0.2686, mean=0.489, std=0.098, med=0.521, p10=0.335, p90=0.589, p_hat=0.524\n",
            "       bin  count   mean_p  emp_rate\n",
            "(0.2, 0.3]    975 0.260040  0.017436\n",
            "(0.3, 0.4]   2147 0.356426  0.027946\n",
            "(0.4, 0.5]   3248 0.452873  0.184729\n",
            "(0.5, 0.6]   8208 0.551017  0.795078\n",
            "(0.6, 0.7]    943 0.610439  0.984093\n",
            "val orig: Brier=0.248301, Base=0.249879, BSS=0.0063, ECE=0.0158, mean=0.512, std=0.032, med=0.509, p10=0.477, p90=0.557, p_hat=0.511\n",
            "       bin  count   mean_p  emp_rate\n",
            "(0.3, 0.4]      5 0.388744  0.000000\n",
            "(0.4, 0.5]   1194 0.480666  0.501675\n",
            "(0.5, 0.6]   2126 0.529471  0.517404\n",
            "val iso: Brier=0.270081, Base=0.249879, BSS=-0.0808, ECE=0.1350, mean=0.532, std=0.196, med=0.461, p10=0.335, p90=0.822, p_hat=0.511\n",
            "          bin  count   mean_p  emp_rate\n",
            "(-0.001, 0.1]     33 0.072946  0.121212\n",
            "   (0.1, 0.2]    118 0.160360  0.330508\n",
            "   (0.2, 0.3]     92 0.264172  0.478261\n",
            "   (0.3, 0.4]    273 0.338050  0.439560\n",
            "   (0.4, 0.5]   1297 0.437346  0.529684\n",
            "   (0.5, 0.6]    608 0.561039  0.504934\n",
            "   (0.6, 0.7]    123 0.620469  0.512195\n",
            "   (0.7, 0.8]    218 0.729104  0.509174\n",
            "   (0.8, 0.9]    340 0.830167  0.497059\n",
            "   (0.9, 1.0]    223 0.925079  0.695067\n",
            "val qcal: Brier=0.248301, Base=0.249879, BSS=0.0063, ECE=0.0140, mean=0.512, std=0.033, med=0.509, p10=0.475, p90=0.559, p_hat=0.511\n",
            "       bin  count   mean_p  emp_rate\n",
            "(0.3, 0.4]      5 0.385800  0.000000\n",
            "(0.4, 0.5]   1247 0.480191  0.497995\n",
            "(0.5, 0.6]   2073 0.530787  0.520019\n",
            "test orig: Brier=0.251163, Base=0.247644, BSS=-0.0142, ECE=0.0475, mean=0.501, std=0.023, med=0.501, p10=0.472, p90=0.529, p_hat=0.549\n",
            "       bin  count   mean_p  emp_rate\n",
            "(0.4, 0.5]   1566 0.481904  0.563857\n",
            "(0.5, 0.6]   1761 0.517975  0.534923\n",
            "test iso: Brier=0.277454, Base=0.247644, BSS=-0.1204, ECE=0.1734, mean=0.460, std=0.138, med=0.434, p10=0.271, p90=0.620, p_hat=0.549\n",
            "          bin  count   mean_p  emp_rate\n",
            "(-0.001, 0.1]      4 0.084980  1.000000\n",
            "   (0.1, 0.2]    206 0.167801  0.480583\n",
            "   (0.2, 0.3]    147 0.264947  0.557823\n",
            "   (0.3, 0.4]    270 0.338777  0.648148\n",
            "   (0.4, 0.5]   1694 0.438640  0.580874\n",
            "   (0.5, 0.6]    642 0.560360  0.454829\n",
            "   (0.6, 0.7]    120 0.620469  0.416667\n",
            "   (0.7, 0.8]    156 0.719355  0.461538\n",
            "   (0.8, 0.9]     66 0.825614  0.696970\n",
            "   (0.9, 1.0]     22 0.935740  0.954545\n",
            "test qcal: Brier=0.25132, Base=0.247644, BSS=-0.0148, ECE=0.0481, mean=0.500, std=0.024, med=0.501, p10=0.470, p90=0.530, p_hat=0.549\n",
            "       bin  count   mean_p  emp_rate\n",
            "(0.4, 0.5]   1612 0.481136  0.562655\n",
            "(0.5, 0.6]   1715 0.518524  0.535277\n"
          ]
        }
      ],
      "source": [
        "# Probability calibration diagnostics for prob_up variants (orig, isotonic, per-quantile)\n",
        "import pandas as pd\n",
        "\n",
        "def eval_probs(df: pd.DataFrame, name: str, columns=(('orig','prob_up'), ('iso','prob_up_cal'), ('qcal','prob_up_qcal'))):\n",
        "    y = pd.to_numeric(df['y_true'], errors='coerce')\n",
        "    ybin = (y > 0).astype(int)\n",
        "    for label, col in columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"{name} {label}: '{col}' missing, skip\")\n",
        "            continue\n",
        "        p = pd.to_numeric(df[col], errors='coerce')\n",
        "        m = p.notna() & ybin.notna()\n",
        "        if not m.any():\n",
        "            print(f\"{name} {label}: no valid rows\")\n",
        "            continue\n",
        "        br = brier_score(p[m], ybin[m])\n",
        "        br_base, p_hat = base_rate_brier(ybin[m])\n",
        "        bss = (1.0 - br / br_base) if (br_base and br_base > 0) else float('nan')\n",
        "        rel, ece = reliability_bins(p[m], ybin[m], num_bins=10)\n",
        "        desc = p[m].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])\n",
        "        print(\n",
        "            f\"{name} {label}: Brier={br:.6g}, Base={br_base:.6g}, BSS={bss:.4f}, ECE={ece:.4f}, \"\n",
        "            f\"mean={desc['mean']:.3f}, std={desc['std']:.3f}, med={desc['50%']:.3f}, \"\n",
        "            f\"p10={desc['10%']:.3f}, p90={desc['90%']:.3f}, p_hat={p_hat:.3f}\"\n",
        "        )\n",
        "        # Show first few reliability bins\n",
        "        try:\n",
        "            print(rel.head(10).to_string(index=False))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "print(\"Calibrated probability diagnostics (orig vs isotonic vs per-quantile):\")\n",
        "eval_probs(train_df, 'train')\n",
        "eval_probs(val_df, 'val')\n",
        "eval_probs(test_df, 'test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trade summary (test_ind): overall\n",
            " total_rows  num_trades  num_longs  num_shorts  trades_share  long_share  short_share  win_rate  avg_trade_ret  median_trade_ret\n",
            "       3313          31         31           0      0.009357         1.0          0.0  0.741935       0.048173          0.072114\n",
            "\n",
            "Quantiles (first 9 rows):\n",
            " quantile       all      long  short\n",
            "     0.01 -0.041478 -0.041478    NaN\n",
            "     0.05 -0.033795 -0.033795    NaN\n",
            "     0.10 -0.025776 -0.025776    NaN\n",
            "     0.25 -0.000186 -0.000186    NaN\n",
            "     0.50  0.072114  0.072114    NaN\n",
            "     0.75  0.083455  0.083455    NaN\n",
            "     0.90  0.089075  0.089075    NaN\n",
            "     0.95  0.094328  0.094328    NaN\n",
            "     0.99  0.096032  0.096032    NaN\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'overall':    total_rows  num_trades  num_longs  num_shorts  trades_share  long_share  \\\n",
              " 0        3313          31         31           0      0.009357         1.0   \n",
              " \n",
              "    short_share  win_rate  avg_trade_ret  median_trade_ret  \n",
              " 0          0.0  0.741935       0.048173          0.072114  ,\n",
              " 'by_side':     side  count  win_rate      mean    median       std       min       p05  \\\n",
              " 0   long     31  0.741935  0.048173  0.072114  0.045769 -0.043192 -0.033795   \n",
              " 1  short      0       NaN       NaN       NaN       NaN       NaN       NaN   \n",
              " \n",
              "         p10       p25       p50       p75       p90       p95       max  \\\n",
              " 0 -0.025776 -0.000186  0.072114  0.083455  0.089075  0.094328  0.096368   \n",
              " 1       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
              " \n",
              "        es_5  top5_mean  \n",
              " 0 -0.043192   0.096368  \n",
              " 1       NaN        NaN  ,\n",
              " 'quantiles':    quantile       all      long  short\n",
              " 0      0.01 -0.041478 -0.041478    NaN\n",
              " 1      0.05 -0.033795 -0.033795    NaN\n",
              " 2      0.10 -0.025776 -0.025776    NaN\n",
              " 3      0.25 -0.000186 -0.000186    NaN\n",
              " 4      0.50  0.072114  0.072114    NaN\n",
              " 5      0.75  0.083455  0.083455    NaN\n",
              " 6      0.90  0.089075  0.089075    NaN\n",
              " 7      0.95  0.094328  0.094328    NaN\n",
              " 8      0.99  0.096032  0.096032    NaN,\n",
              " 'by_prob_bin':           bin  trades  mean_ret  median_ret  win_rate  long_share  \\\n",
              " 0  (0.5, 0.6]      31  0.048173    0.072114  0.741935         1.0   \n",
              " \n",
              "    short_share       p05       p95  \n",
              " 0          0.0 -0.033795  0.094328  ,\n",
              " 'by_exp_bin':                   bin  trades  mean_ret  median_ret  win_rate  long_share  \\\n",
              " 0   (0.0066, 0.00795]       4 -0.001692   -0.000975  0.500000         1.0   \n",
              " 1  (0.00795, 0.00833]       3  0.022725   -0.002834  0.333333         1.0   \n",
              " 2  (0.00833, 0.00846]       3  0.074867    0.072114  1.000000         1.0   \n",
              " 3  (0.00846, 0.00856]       3  0.081232    0.083223  1.000000         1.0   \n",
              " 4  (0.00856, 0.00857]       3  0.041759    0.073254  0.666667         1.0   \n",
              " 5  (0.00857, 0.00866]       3  0.002161   -0.037480  0.333333         1.0   \n",
              " 6  (0.00866, 0.00883]       3  0.049129    0.084088  0.666667         1.0   \n",
              " 7   (0.00883, 0.0101]       3  0.062569    0.064204  1.000000         1.0   \n",
              " 8    (0.0101, 0.0106]       3  0.074809    0.083673  1.000000         1.0   \n",
              " 9     (0.0106, 0.011]       3  0.090794    0.095249  1.000000         1.0   \n",
              " \n",
              "    short_share       p05       p95  \n",
              " 0          0.0 -0.008322  0.003933  \n",
              " 1          0.0 -0.005720  0.069062  \n",
              " 2          0.0  0.067731  0.083931  \n",
              " 3          0.0  0.077835  0.083236  \n",
              " 4          0.0 -0.015873  0.077344  \n",
              " 5          0.0 -0.042621  0.074690  \n",
              " 6          0.0 -0.018690  0.092476  \n",
              " 7          0.0  0.056500  0.067495  \n",
              " 8          0.0  0.054880  0.088535  \n",
              " 9          0.0  0.082213  0.096256  }"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Detailed trade distribution summary for indicator-based signals\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def summarize_trade_distribution(\n",
        "    df: pd.DataFrame,\n",
        "    signal_col: str = 'signal_prob_exp_q50',\n",
        "    y_col: str = 'y_true',\n",
        "    prob_col: str | None = 'prob_up',\n",
        "    exp_col: str | None = 'exp_ret_avg',\n",
        "    num_prob_bins: int = 10,\n",
        "    num_exp_bins: int = 10,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a detailed summary of simulated trades induced by a {-1,0,1} signal column.\n",
        "\n",
        "    Returns a dict of DataFrames:\n",
        "      - 'overall': single-row overall stats\n",
        "      - 'by_side': stats for long and short subsets\n",
        "      - 'quantiles': return quantiles for all/long/short\n",
        "      - 'by_prob_bin': per-bin stats by prob_col (if available)\n",
        "      - 'by_exp_bin': per-bin stats by exp_col (if available)\n",
        "    \"\"\"\n",
        "    if signal_col not in df.columns:\n",
        "        raise KeyError(f\"'{signal_col}' not found\")\n",
        "    if y_col not in df.columns:\n",
        "        raise KeyError(f\"'{y_col}' not found\")\n",
        "\n",
        "    s = pd.to_numeric(df[signal_col], errors='coerce').fillna(0).astype(int)\n",
        "    y_log = pd.to_numeric(df[y_col], errors='coerce')\n",
        "    simple_ret = np.exp(y_log) - 1.0\n",
        "    trade_ret = s * simple_ret\n",
        "    trades_mask = s != 0\n",
        "\n",
        "    # Wins among trades\n",
        "    wins_mask = trades_mask & y_log.notna()\n",
        "    wins = ((np.sign(y_log[wins_mask]) == s[wins_mask]) & (np.sign(y_log[wins_mask]) != 0)).astype(float)\n",
        "\n",
        "    # Helper to compute stats safely\n",
        "    def _stats(ret: pd.Series, mask: pd.Series):\n",
        "        rr = ret[mask].dropna()\n",
        "        if rr.empty:\n",
        "            return dict(count=0, win_rate=np.nan, mean=np.nan, median=np.nan, std=np.nan,\n",
        "                        min=np.nan, p05=np.nan, p10=np.nan, p25=np.nan, p50=np.nan,\n",
        "                        p75=np.nan, p90=np.nan, p95=np.nan, max=np.nan,\n",
        "                        es_5=np.nan, top5_mean=np.nan)\n",
        "        q = rr.quantile([0.05,0.10,0.25,0.50,0.75,0.90,0.95])\n",
        "        k = max(int(len(rr) * 0.05), 1)\n",
        "        es_5 = rr.nsmallest(k).mean()\n",
        "        top5 = rr.nlargest(k).mean()\n",
        "        return dict(\n",
        "            count=int(len(rr)),\n",
        "            win_rate=float(((np.sign(y_log[mask]) == s[mask]) & (np.sign(y_log[mask]) != 0)).mean()) if mask.any() else np.nan,\n",
        "            mean=float(rr.mean()),\n",
        "            median=float(rr.median()),\n",
        "            std=float(rr.std(ddof=0)),\n",
        "            min=float(rr.min()),\n",
        "            p05=float(q.loc[0.05]),\n",
        "            p10=float(q.loc[0.10]),\n",
        "            p25=float(q.loc[0.25]),\n",
        "            p50=float(q.loc[0.50]),\n",
        "            p75=float(q.loc[0.75]),\n",
        "            p90=float(q.loc[0.90]),\n",
        "            p95=float(q.loc[0.95]),\n",
        "            max=float(rr.max()),\n",
        "            es_5=float(es_5),\n",
        "            top5_mean=float(top5),\n",
        "        )\n",
        "\n",
        "    overall = {\n",
        "        'total_rows': int(len(df)),\n",
        "        'num_trades': int(trades_mask.sum()),\n",
        "        'num_longs': int((s == 1).sum()),\n",
        "        'num_shorts': int((s == -1).sum()),\n",
        "        'trades_share': float(trades_mask.mean()),\n",
        "        'long_share': float(((s == 1).sum()) / trades_mask.sum()) if trades_mask.sum() else np.nan,\n",
        "        'short_share': float(((s == -1).sum()) / trades_mask.sum()) if trades_mask.sum() else np.nan,\n",
        "        'win_rate': float(wins.mean()) if len(wins) else np.nan,\n",
        "        'avg_trade_ret': float(trade_ret[trades_mask].mean()) if trades_mask.any() else np.nan,\n",
        "        'median_trade_ret': float(trade_ret[trades_mask].median()) if trades_mask.any() else np.nan,\n",
        "    }\n",
        "    overall_df = pd.DataFrame([overall])\n",
        "\n",
        "    by_side_rows = []\n",
        "    by_side_rows.append({'side': 'long', **_stats(trade_ret, s == 1)})\n",
        "    by_side_rows.append({'side': 'short', **_stats(trade_ret, s == -1)})\n",
        "    by_side_df = pd.DataFrame(by_side_rows)\n",
        "\n",
        "    # Quantiles table for all / long / short\n",
        "    qs = [0.01,0.05,0.10,0.25,0.50,0.75,0.90,0.95,0.99]\n",
        "    def _quantiles(series: pd.Series):\n",
        "        if series.dropna().empty:\n",
        "            return pd.Series({q: np.nan for q in qs})\n",
        "        return series.quantile(qs)\n",
        "    quantiles_df = pd.DataFrame({\n",
        "        'all': _quantiles(trade_ret[trades_mask]),\n",
        "        'long': _quantiles(trade_ret[s == 1]),\n",
        "        'short': _quantiles(trade_ret[s == -1]),\n",
        "    })\n",
        "    quantiles_df.index.name = 'quantile'\n",
        "    quantiles_df = quantiles_df.reset_index()\n",
        "\n",
        "    # Per-bin by probability\n",
        "    by_prob_bin = pd.DataFrame()\n",
        "    if prob_col and prob_col in df.columns:\n",
        "        p = pd.to_numeric(df[prob_col], errors='coerce')\n",
        "        tm = trades_mask & p.notna()\n",
        "        tmp = pd.DataFrame({\n",
        "            'p': p[tm],\n",
        "            'ret': trade_ret[tm],\n",
        "            'win': ((np.sign(y_log[tm]) == s[tm]) & (np.sign(y_log[tm]) != 0)).astype(float),\n",
        "            'side': s[tm],\n",
        "        })\n",
        "        edges = np.linspace(0.0, 1.0, int(num_prob_bins) + 1)\n",
        "        tmp['bin'] = pd.cut(tmp['p'], bins=edges, include_lowest=True, right=True)\n",
        "        g = tmp.groupby('bin', observed=True)\n",
        "        by_prob_bin = g.agg(\n",
        "            trades=('ret', 'size'),\n",
        "            mean_ret=('ret', 'mean'),\n",
        "            median_ret=('ret', 'median'),\n",
        "            win_rate=('win', 'mean'),\n",
        "            long_share=('side', lambda a: float((a == 1).mean()) if len(a) else np.nan),\n",
        "            short_share=('side', lambda a: float((a == -1).mean()) if len(a) else np.nan),\n",
        "            p05=('ret', lambda a: float(np.quantile(a, 0.05)) if len(a) else np.nan),\n",
        "            p95=('ret', lambda a: float(np.quantile(a, 0.95)) if len(a) else np.nan),\n",
        "        ).reset_index()\n",
        "\n",
        "    # Per-bin by expected return (quantile bins)\n",
        "    by_exp_bin = pd.DataFrame()\n",
        "    if exp_col and exp_col in df.columns:\n",
        "        x = pd.to_numeric(df[exp_col], errors='coerce')\n",
        "        tm = trades_mask & x.notna()\n",
        "        tmp = pd.DataFrame({\n",
        "            'x': x[tm],\n",
        "            'ret': trade_ret[tm],\n",
        "            'win': ((np.sign(y_log[tm]) == s[tm]) & (np.sign(y_log[tm]) != 0)).astype(float),\n",
        "            'side': s[tm],\n",
        "        })\n",
        "        try:\n",
        "            tmp['bin'] = pd.qcut(tmp['x'], q=min(int(num_exp_bins), max(1, tmp['x'].nunique())), duplicates='drop')\n",
        "            g = tmp.groupby('bin', observed=True)\n",
        "            by_exp_bin = g.agg(\n",
        "                trades=('ret', 'size'),\n",
        "                mean_ret=('ret', 'mean'),\n",
        "                median_ret=('ret', 'median'),\n",
        "                win_rate=('win', 'mean'),\n",
        "                long_share=('side', lambda a: float((a == 1).mean()) if len(a) else np.nan),\n",
        "                short_share=('side', lambda a: float((a == -1).mean()) if len(a) else np.nan),\n",
        "                p05=('ret', lambda a: float(np.quantile(a, 0.05)) if len(a) else np.nan),\n",
        "                p95=('ret', lambda a: float(np.quantile(a, 0.95)) if len(a) else np.nan),\n",
        "            ).reset_index()\n",
        "        except Exception:\n",
        "            by_exp_bin = pd.DataFrame()\n",
        "\n",
        "    return {\n",
        "        'overall': overall_df,\n",
        "        'by_side': by_side_df,\n",
        "        'quantiles': quantiles_df,\n",
        "        'by_prob_bin': by_prob_bin,\n",
        "        'by_exp_bin': by_exp_bin,\n",
        "    }\n",
        "\n",
        "# Example: summarize distribution for test_ind if available\n",
        "try:\n",
        "    trade_summary_test = summarize_trade_distribution(\n",
        "        test_ind,\n",
        "        signal_col='signal_prob_exp_q50',\n",
        "        y_col='y_true',\n",
        "        prob_col='prob_up' if 'prob_up' in test_ind.columns else None,\n",
        "        exp_col='exp_ret_avg' if 'exp_ret_avg' in test_ind.columns else None,\n",
        "    )\n",
        "    print(\"Trade summary (test_ind): overall\")\n",
        "    print(trade_summary_test['overall'].to_string(index=False))\n",
        "    print(\"\\nQuantiles (first 9 rows):\")\n",
        "    print(trade_summary_test['quantiles'].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Trade summary (test_ind) skipped:\", e)\n",
        "\n",
        "summarize_trade_distribution(test_ind)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
